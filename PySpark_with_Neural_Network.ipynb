{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark with Neural Networks\n",
        "Pyspark implementation of Shallow Neural Network from scratch using MNIST.\n",
        "\n",
        "The goal of this project is to use Spark to build a 3 layer Neural Network from Scratch, using my own mathematical formulas to solve an Image classification Task. I will use the very famous MNIST dataset"
      ],
      "metadata": {
        "id": "srTseLAut4cD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Dataset"
      ],
      "metadata": {
        "id": "a3gpvLb-t1AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import sys\n",
        "import pyspark\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "from pyspark import SparkContext as sc\n",
        "\n",
        "print(\"Pyspark Script:\", sys.argv[0])\n",
        "print(\"PySpark version:\", pyspark.__version__)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = sc.getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJyizVz4tzus",
        "outputId": "98f6abbb-1474-4f77-9cff-83b57cc4f3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pyspark Script: /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\n",
            "PySpark version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Start downloading dataset...')\n",
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdI5YqVTtzsT",
        "outputId": "05e709f1-db26-4905-930b-9865f6993c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = to_categorical(y_train).reshape(-1, 10)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "36QEK37Stzp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('data/'):\n",
        "    os.makedirs('data/')\n",
        "\n",
        "\n",
        "np.savetxt('data/mnist_images_train.csv', x_train.reshape(len(x_train),784).tolist())\n",
        "np.savetxt('data/mnist_images_test.csv', x_test.reshape(len(x_test),784).tolist())\n",
        "np.savetxt('data/mnist_labels_train.csv', y_train.tolist())\n",
        "np.savetxt('data/mnist_labels_test.csv', y_test.tolist())\n",
        "\n",
        "print('Dataset downloaded.')\n",
        "\n",
        "print('Data is located here:', os.getcwd() + '\\data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-pejRw1tznb",
        "outputId": "a0428bef-9f21-43bc-c086-45df68a7832e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded.\n",
            "Data is located here: /content\\data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "2iE53Mp5w2CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_train_images = sc.textFile(\"data/mnist_images_train.csv\", 1)\n",
        "x_train = txt_train_images.map(lambda x : np.fromstring(x, dtype=float, sep=' ').reshape(1, 784)).zipWithIndex().map(lambda x: (str(x[1]), x[0]))\n",
        "\n",
        "txt_train_labels = sc.textFile(\"data/mnist_labels_train.csv\", 1)\n",
        "y_train = txt_train_labels.map(lambda x : np.fromstring(x, dtype=float, sep=' ').reshape(1, 10)).zipWithIndex().map(lambda x: (str(x[1]), x[0]))\n",
        "\n",
        "txt_test_images = sc.textFile(\"data/mnist_images_test.csv\", 1)\n",
        "x_test = txt_test_images.map(lambda x : np.fromstring(x, dtype=float, sep=' ').reshape(1, 784)).zipWithIndex().map(lambda x: (str(x[1]), x[0]))\n",
        "\n",
        "txt_test_labels = sc.textFile(\"data/mnist_labels_test.csv\", 1)\n",
        "y_test = txt_test_labels.map(lambda x : np.fromstring(x, dtype=float, sep=' ').reshape(1, 10)).zipWithIndex().map(lambda x: (str(x[1]), x[0]))"
      ],
      "metadata": {
        "id": "PMb49ekctzlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_rdd = x_train.join(y_train).map(lambda x: x[1])\n",
        "test_ds_rdd = x_test.join(y_test).map(lambda x: x[1])\n",
        "\n",
        "# train_rdd = train_ds_rdd.filter(lambda x: np.array_equal(x[1][0], [1., 0.]) or np.array_equal(x[1][0], [0., 1.]))\n",
        "# test_rdd = test_ds_rdd.filter(lambda x: np.array_equal(x[1][0], [1., 0.]) or np.array_equal(x[1][0], [0., 1.]))\n",
        "\n",
        "train_rdd = train_ds_rdd\n",
        "test_rdd = test_ds_rdd\n",
        "\n",
        "train_rdd.cache()\n",
        "test_rdd.cache()\n",
        "# print(train_rdd.take(1))\n",
        "print(\"Trainset size:\", train_rdd.count())\n",
        "print(\"Testset size:\", test_ds_rdd.count())"
      ],
      "metadata": {
        "id": "e1ZACdsHtzii"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_rdd = x_train.join(y_train).map(lambda x: x[1])\n",
        "# test_rdd = x_test.join(y_test).map(lambda x: x[1])\n",
        "# train_rdd.cache()\n",
        "# test_rdd.cache()\n",
        "\n",
        "# print(\"Trainset size:\", train_rdd.count())\n",
        "# print(\"Testset size:\", test_rdd.count())\n",
        "\n",
        "# print('Data Loaded!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6TbqShqtzdT",
        "outputId": "7843fc52-7174-40fc-aa4e-ac52f5b1ada6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Functions"
      ],
      "metadata": {
        "id": "pY4HxrZO0xoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Functions"
      ],
      "metadata": {
        "id": "ZcPJkzg80zlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # General function to apply any activation function\n",
        "def activation(x, f):\n",
        "    return f(x)\n",
        "\n",
        "# # Sigmoid Activation function\n",
        "# def sigmoid(X):\n",
        "#     return 1 / (1 + np.exp(-X))\n",
        "\n",
        "# # Sigmoid prime function (used for backward prop)\n",
        "# def sigmoid_prime(x):\n",
        "#     sig = sigmoid(x)\n",
        "#     return sig * (1 - sig)\n",
        "\n",
        "# --- Neural Network Functions for Multiclass ---\n",
        "def softmax(z):\n",
        "    e_z = np.exp(z - np.max(z))\n",
        "    return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_pred, y_true, eps=1e-8):\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return (x > 0).astype(float)"
      ],
      "metadata": {
        "id": "sesms0J1tzRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation"
      ],
      "metadata": {
        "id": "7cY6iHqs05Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the layer propagation before activation\n",
        "def preforward(x, w, b):\n",
        "    return np.dot(x, w) + b\n",
        "\n",
        "# Compute the layer propagation after activation\n",
        "# This is also equivalent to a predict function once model is trained\n",
        "# def predict(x, W1, B1, W2, B2):\n",
        "#     return sigmoid(preforward(sigmoid(preforward(x , W1, B1)), W2, B2))\n",
        "\n",
        "def predict(x, W1, B1, W2, B2):\n",
        "    h1 = relu(preforward(x, W1, B1))\n",
        "    out = softmax(preforward(h1, W2, B2))\n",
        "    return out"
      ],
      "metadata": {
        "id": "Z4WU1ZCjtzPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Propagation"
      ],
      "metadata": {
        "id": "6odLZBFK095C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Compute the derivative of the error regarding B2\n",
        "# def derivativeB2(y_pred, y_true, y_h, f_prime):\n",
        "#     return (y_pred - y_true) * f_prime(y_h)\n",
        "\n",
        "# # Compute the derivative of the error regarding W2\n",
        "# def derivativeW2(h, dB2):\n",
        "#     return np.dot(h.T, dB2)\n",
        "\n",
        "# # Compute the derivative of the error regarding B1\n",
        "# def derivativeB1(h_h, dB2, W2, f_prime):\n",
        "#     return np.dot(dB2, W2.T) * f_prime(h_h)\n",
        "\n",
        "# # Compute the derivative of the error regarding W1\n",
        "# def derivativeW1(x, dB1):\n",
        "#     return np.dot(x.T, dB1)\n",
        "\n",
        "def derivativeB2(y_pred, y_true):\n",
        "    # dL/dB2 = y_pred - y_true\n",
        "    return y_pred - y_true  # shape: (1, 10)\n",
        "\n",
        "def derivativeW2(h, dB2):\n",
        "    # dL/dW2 = h.T @ (y_pred - y_true)\n",
        "    return np.dot(h.T, dB2)  # shapes: (64, 1)T @ (1, 10) => (64, 10)\n",
        "\n",
        "def derivativeB1(dB2, W2, h_h):\n",
        "    # dL/dB1 = (y_pred - y_true) @ W2.T * relu'(h_h)\n",
        "    return np.dot(dB2, W2.T) * relu_prime(h_h)\n",
        "\n",
        "def derivativeW1(x, dB1):\n",
        "    # dL/dW1 = x.T @ dB1\n",
        "    return np.dot(x.T, dB1)"
      ],
      "metadata": {
        "id": "9uguSvVytzMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "vLce8Av71CWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(pred, true):\n",
        "    cm = multilabel_confusion_matrix(true, pred)\n",
        "    return (cm)\n",
        "\n",
        "# Cost function\n",
        "def sse(y_pred, y_true):\n",
        "    return 0.5 * np.sum(np.power(y_pred - y_true, 2))\n",
        "\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# def get_metrics(pred, true):\n",
        "#     pred_labels = np.argmax(pred, axis=1)\n",
        "#     true_labels = np.argmax(true, axis=1)\n",
        "#     cm = confusion_matrix(true_labels, pred_labels)\n",
        "#     return cm"
      ],
      "metadata": {
        "id": "D7KoJcCItzKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Mini Batch"
      ],
      "metadata": {
        "id": "BMABCNrO16-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_iteration = 50\n",
        "learningRate = 0.1\n",
        "\n",
        "input_layer = 784 # number of neurones in the input layer (equal to image size)\n",
        "hidden_layer = 64 # number of neurones in the hidden layer (Custom)\n",
        "output_layer = 10 # number of neurones in the output layer (equal to the number of possible labels)\n",
        "\n",
        "# Paramater Initialization\n",
        "W1 = np.random.rand(input_layer, hidden_layer) - 0.5 # Shape (784, 64)\n",
        "W2 = np.random.rand(hidden_layer, output_layer) - 0.5 # Shape (64, 10)\n",
        "B1 = np.random.rand(1, hidden_layer) - 0.5 # Shape (1, 64)\n",
        "B2 = np.random.rand(1, output_layer) - 0.5 # Shape (1, 10)\n",
        "\n",
        "# History over epochs\n",
        "cost_history = []\n",
        "acc_history = []"
      ],
      "metadata": {
        "id": "KnZo-NRktzH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Epoch Loop (mini batch implementation)\n",
        "# print(\"Start Training Loop:\")\n",
        "\n",
        "# for i in range(num_iteration):\n",
        "\n",
        "#     # Compute gradients, cost and accuracy over mini batch\n",
        "\n",
        "#     ################## Notations ######################\n",
        "#     # x -> Input Image flatten of shape (1, 784)\n",
        "#     # y* -> One hot label of shape (1, 2)\n",
        "#     # h^ -> Forward prop from Input layer to hidden layer before activation (1, 64) using W1, B1 parm\n",
        "#     # h -> Forward prop from Input layer to hidden layer after tanh activation (1, 64)\n",
        "#     # y^ -> Forward prop from hidden layer to output layer before activation (1, 2) using W2, B2 parm\n",
        "#     # y -> Forward prop from hidden layer to output layer after sigmoid activation (1, 2)\n",
        "#     # E -> Error between y and y* using SSE\n",
        "#     # Acc -> 1 is right prediction 0 otherwise\n",
        "#     # DE/D? -> Partial derivative of the Error regarding parmaters (B2, W2, B1, W1)\n",
        "\n",
        "\n",
        "#     ################# Forward Prop ######################\n",
        "#     # map batch ([x], [y*]) to ([x], [h^],[y*])\n",
        "#     # map batch ([x], [h^],[y*]) to ([x], [h^], [h], [y*])\n",
        "#     # map batch ([x], [h^], [h], [y*]) to ([x], [h^], [h], [y^], [y*])\n",
        "#     # map batch ([x], [h^], [h], [y^], [y*]) to ([x], [h^], [h], [y^], [y], [y*])\n",
        "#     ################# Backward Prop #####################\n",
        "#     # map batch ([x], [h^], [h], [y^], [y], [y*]) to ([x], [h^], [h], [E], [DE/DB2], [Acc])\n",
        "#     # map batch ([x], [h^], [h], [E], [DE/DB2], [Acc]) to ([x], [h^], [E], [DE/DB2], [DE/DW2], [Acc])\n",
        "#     # map batch ([x], [h^], [E], [DE/DB2], [DE/DW2], [Acc]) to ([x], [E], [DE/DB2], [DE/DW2], [DE/DB1], [Acc])\n",
        "#     # map batch ([x], [E], [DE/DB2], [DE/DW2], [DE/DB1], [Acc]) to ([E], [DE/DB2], [DE/DW2], [DE/DB1], [DE/DW1],[Acc])\n",
        "#     ############### Reduce over the mini batch #########\n",
        "\n",
        "\n",
        "    # gradientCostAcc = train_rdd\\\n",
        "    #                     .sample(False,0.7)\\\n",
        "    #                     .map(lambda x: (x[0], preforward(x[0], W1, B1), x[1]))\\\n",
        "    #                     .map(lambda x: (x[0], x[1], activation(x[1], sigmoid), x[2]))\\\n",
        "    #                     .map(lambda x: (x[0], x[1], x[2], preforward(x[2], W2, B2), x[3]))\\\n",
        "    #                     .map(lambda x: (x[0], x[1], x[2], x[3], activation(x[3], sigmoid), x[4]))\\\n",
        "    #                     .map(lambda x: (x[0], x[1], x[2], sse(x[4], x[5]), derivativeB2(x[4], x[5], x[3], sigmoid_prime), int(np.argmax(x[4]) == np.argmax(x[5]))))\\\n",
        "    #                     .map(lambda x: (x[0], x[1], x[3], x[4],  derivativeW2(x[2], x[4]) ,x[5]))\\\n",
        "    #                     .map(lambda x: (x[0], x[2], x[3], x[4],  derivativeB1(x[1],  x[3], W2, sigmoid_prime) ,x[5]))\\\n",
        "    #                     .map(lambda x: (x[1], x[2], x[3], x[4], derivativeW1(x[0], x[4]) ,x[5], 1)) \\\n",
        "    #                     .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4], x[5] + y[5], x[6] + y[6]))\n",
        "\n",
        "#     # Cost and Accuarcy of the mini batch\n",
        "#     n = gradientCostAcc[-1] # number of images in the mini batch\n",
        "#     cost = gradientCostAcc[0]/n # Cost over the mini batch\n",
        "#     acc = gradientCostAcc[5]/n # Accuarcy over the mini batch\n",
        "\n",
        "#      # Add to history\n",
        "#     cost_history.append(cost)\n",
        "#     acc_history.append(acc)\n",
        "\n",
        "\n",
        "#     # Extract gradiends\n",
        "#     DB2 = gradientCostAcc[1]/n\n",
        "#     DW2 = gradientCostAcc[2]/n\n",
        "#     DB1 = gradientCostAcc[3]/n\n",
        "#     DW1 = gradientCostAcc[4]/n\n",
        "\n",
        "#     # Update parameter with new learning rate and gradients using Gradient Descent\n",
        "#     B2 -= learningRate * DB2\n",
        "#     W2 -= learningRate * DW2\n",
        "#     B1 -= learningRate * DB1\n",
        "#     W1 -= learningRate * DW1\n",
        "\n",
        "#     # Display performances\n",
        "#     print(f\"   Epoch {i+1}/{num_iteration} | Cost: {cost_history[i]} | Acc: {acc_history[i]*100} | Batchsize:{n}\")\n",
        "\n",
        "# print(\"Training end..\")\n",
        "\n",
        "# # Plot of cost over epochs\n",
        "# plt.subplot(2, 1, 1)\n",
        "# plt.plot(cost_history)\n",
        "# plt.show()\n",
        "\n",
        "# # Plot of accuracy over epochs\n",
        "# plt.subplot(2, 1, 2)\n",
        "# plt.plot(acc_history)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hAu6QhGCtzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch Loop (mini batch implementation)\n",
        "print(\"Start Training Loop:\")\n",
        "\n",
        "for i in range(num_iteration):\n",
        "\n",
        "    # Compute gradients, cost and accuracy over mini batch\n",
        "\n",
        "    ################## Notations ######################\n",
        "    # x -> Input Image flatten of shape (1, 784)\n",
        "    # y* -> One hot label of shape (1, 2)\n",
        "    # h^ -> Forward prop from Input layer to hidden layer before activation (1, 64) using W1, B1 parm\n",
        "    # h -> Forward prop from Input layer to hidden layer after tanh activation (1, 64)\n",
        "    # y^ -> Forward prop from hidden layer to output layer before activation (1, 2) using W2, B2 parm\n",
        "    # y -> Forward prop from hidden layer to output layer after sigmoid activation (1, 2)\n",
        "    # E -> Error between y and y* using SSE\n",
        "    # Acc -> 1 is right prediction 0 otherwise\n",
        "    # DE/D? -> Partial derivative of the Error regarding parmaters (B2, W2, B1, W1)\n",
        "\n",
        "\n",
        "    ################# Forward Prop ######################\n",
        "    # map batch ([x], [y*]) to ([x], [h^],[y*])\n",
        "    # map batch ([x], [h^],[y*]) to ([x], [h^], [h], [y*])\n",
        "    # map batch ([x], [h^], [h], [y*]) to ([x], [h^], [h], [y^], [y*])\n",
        "    # map batch ([x], [h^], [h], [y^], [y*]) to ([x], [h^], [h], [y^], [y], [y*])\n",
        "    ################# Backward Prop #####################\n",
        "    # map batch ([x], [h^], [h], [y^], [y], [y*]) to ([x], [h^], [h], [E], [DE/DB2], [Acc])\n",
        "    # map batch ([x], [h^], [h], [E], [DE/DB2], [Acc]) to ([x], [h^], [E], [DE/DB2], [DE/DW2], [Acc])\n",
        "    # map batch ([x], [h^], [E], [DE/DB2], [DE/DW2], [Acc]) to ([x], [E], [DE/DB2], [DE/DW2], [DE/DB1], [Acc])\n",
        "    # map batch ([x], [E], [DE/DB2], [DE/DW2], [DE/DB1], [Acc]) to ([E], [DE/DB2], [DE/DW2], [DE/DB1], [DE/DW1],[Acc])\n",
        "    ############### Reduce over the mini batch #########\n",
        "\n",
        "\n",
        "    gradientCostAcc = train_rdd\\\n",
        "                        .sample(False,0.7)\\\n",
        "                        .map(lambda x: (x[0], preforward(x[0], W1, B1), x[1]))\\\n",
        "                        .map(lambda x: (x[0], x[1], activation(x[1], relu), x[2]))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], preforward(x[2], W2, B2), x[3]))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], x[3], activation(x[3], softmax), x[4]))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], cross_entropy(x[4], x[5]), derivativeB2(x[4], x[5]), int(np.argmax(x[4]) == np.argmax(x[5]))))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], derivativeW2(x[2], x[7])))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], derivativeB1(x[7], W2, x[1])))\\\n",
        "                        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], derivativeW1(x[0], x[10]), 1)) \\\n",
        "                        .reduce(lambda x, y: tuple(a + b if isinstance(a, np.ndarray) else a + b for a, b in zip(x, y)))\n",
        "\n",
        "    print(gradientCostAcc)\n",
        "\n",
        "    #\n",
        "    # Cost and Accuarcy of the mini batch\n",
        "    n = gradientCostAcc[-1] # number of images in the mini batch\n",
        "    cost = gradientCostAcc[6]/n # Cost over the mini batch\n",
        "    acc = gradientCostAcc[8]/n # Accuarcy over the mini batch\n",
        "\n",
        "     # Add to history\n",
        "    cost_history.append(cost)\n",
        "    acc_history.append(acc)\n",
        "\n",
        "\n",
        "    # Extract gradiends\n",
        "    DB2 = gradientCostAcc[7]/n\n",
        "    DW2 = gradientCostAcc[9]/n\n",
        "    DB1 = gradientCostAcc[10]/n\n",
        "    DW1 = gradientCostAcc[11]/n\n",
        "\n",
        "    # Update parameter with new learning rate and gradients using Gradient Descent\n",
        "    B2 -= learningRate * DB2\n",
        "    W2 -= learningRate * DW2\n",
        "    B1 -= learningRate * DB1\n",
        "    W1 -= learningRate * DW1\n",
        "\n",
        "    # Display performances\n",
        "    print(f\"   Epoch {i+1}/{num_iteration} | Cost: {cost_history[i]} | Acc: {acc_history[i]*100} | Batchsize:{n}\")\n",
        "\n",
        "print(\"Training end..\")\n",
        "\n",
        "# Plot of cost over epochs\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(cost_history)\n",
        "plt.show()\n",
        "\n",
        "# Plot of accuracy over epochs\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(acc_history)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fAyDOn7zJa1T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Evaluation"
      ],
      "metadata": {
        "id": "urMK-plE4-Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model over the Testset and get Confusion matrix per class\n",
        "metrics = test_rdd.map(lambda x: get_metrics(np.round(predict(x[0], W1, B1, W2, B2)), x[1]))\\\n",
        "                  .reduce(lambda x, y: x + y)\n",
        "\n",
        "# For each class give TP, FP, FN, TN and precision, and recall, and F1 score\n",
        "for label, label_metrics in enumerate(metrics):\n",
        "\n",
        "    print(f\"\\n---- Digit {label} ------\\n\")\n",
        "    tn, fp, fn, tp = label_metrics.ravel()\n",
        "    print(\"TP:\", tp, \"FP:\", fp, \"FN:\", fn, \"TN:\", tn)\n",
        "\n",
        "    precision = tp / (tp + fp + 0.000001)\n",
        "    print(f\"\\nPrecision : {precision}\")\n",
        "\n",
        "    recall = tp / (tp + fn + 0.000001)\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "    F1 = 2 * (precision * recall) / (precision + recall + 0.000001)\n",
        "    print(f\"F1 score: {F1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57cgOG0CtzDE",
        "outputId": "8436b112-a2c3-4f67-c86e-53d6006661cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Digit 0 ------\n",
            "\n",
            "TP: 952 FP: 40 FN: 28 TN: 1095\n",
            "\n",
            "Precision : 0.9596774183874219\n",
            "Recall: 0.9714285704373178\n",
            "F1 score: 0.9655167404188577\n",
            "\n",
            "---- Digit 1 ------\n",
            "\n",
            "TP: 1127 FP: 20 FN: 8 TN: 960\n",
            "\n",
            "Precision : 0.9825632075130224\n",
            "Recall: 0.992951540975373\n",
            "F1 score: 0.9877295604981019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing Pipeline"
      ],
      "metadata": {
        "id": "EoR_oQ_L5n6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some Images for checking\n",
        "for image_test in test_rdd.map(lambda x: (x[0], predict(x[0], W1, B1, W2, B2), np.argmax(x[1]))).takeSample(False, 15):\n",
        "\n",
        "    pred = np.argmax(image_test[1])\n",
        "    print(f'pred: {pred}, prob: {round(image_test[1][0][pred], 2)} true: {image_test[2]}')\n",
        "    image = np.reshape(image_test[0], (28, 28))\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V6Zb3wWDtzAl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L5ixt3Cqty-K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # converts [0,255] uint8 -> [0,1] float\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "LyM0iBjMty7y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVisionTransformer(nn.Module):\n",
        "    def __init__(self, emb_dim=64, num_heads=4, num_layers=2, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.input_dim = 28       # \"patch\" length: we'll treat each row as a token\n",
        "        self.seq_len = 28         # 28 rows in the image\n",
        "\n",
        "        # Embed each row (token) to emb_dim\n",
        "        self.embedding = nn.Linear(self.input_dim, emb_dim)\n",
        "        # learnable positional encoding, add 1 for the CLS token\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq_len + 1, emb_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=emb_dim, nhead=num_heads, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim)) # learnable [CLS] token\n",
        "\n",
        "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, 1, 28, 28)\n",
        "        x = x.squeeze(1)  # (B, 28, 28)\n",
        "        x = self.embedding(x)  # (B, 28, emb_dim)\n",
        "        # prepend cls_token\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # (B, 1, emb_dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, 29, emb_dim)\n",
        "        x = x + self.pos_emb[:, :x.size(1), :] # add positional encoding\n",
        "        x = self.transformer(x)                # (B, 29, emb_dim)\n",
        "        x = x[:, 0, :]                        # take CLS token output\n",
        "        x = self.classifier(x)                 # (B, num_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8nF8OK1Qty5b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleVisionTransformer().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-EGrtujsty3F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_epochs = 10\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     correct = 0\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device).long()\n",
        "#         logits = model(images)\n",
        "#         loss = loss_fn(logits, labels)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item() * images.size(0)\n",
        "#         preds = logits.argmax(1)\n",
        "#         correct += (preds == labels).sum().item()\n",
        "\n",
        "#     acc = correct / len(train_loader.dataset)\n",
        "#     print(f\"Epoch {epoch+1}: Loss {total_loss / len(train_loader.dataset):.4f}, Acc {acc*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    batch_losses = []\n",
        "    batch_accs = []\n",
        "\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader_tqdm):\n",
        "        images, labels = images.to(device), labels.to(device).long()\n",
        "        logits = model(images)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Optionally, track per-batch stats for visualization/logging\n",
        "        batch_loss = loss.item()\n",
        "        batch_acc = (preds == labels).float().mean().item()\n",
        "        batch_losses.append(batch_loss)\n",
        "        batch_accs.append(batch_acc)\n",
        "\n",
        "        # Update tqdm bar with current stats\n",
        "        train_loader_tqdm.set_postfix(\n",
        "            loss=batch_loss, acc=batch_acc, global_acc=correct / ((batch_idx+1) * images.size(0))\n",
        "        )\n",
        "\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}: Loss {total_loss / len(train_loader.dataset):.4f}, Acc {acc*100:.2f}%\")\n",
        "\n",
        "    # Optionally, plot batch_losses/batch_accs per epoch for finer-grained visualization\n"
      ],
      "metadata": {
        "id": "DCLmVK2lty0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "6d449c31-6d0f-4693-dfb5-016c43775015"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-1145086975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-264535151.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 29, emb_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# add positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# (B, 29, emb_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m                        \u001b[0;31m# take CLS token output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# (B, num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    918\u001b[0m             x = self.norm1(\n\u001b[1;32m    919\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m             )\n\u001b[1;32m    922\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         )[0]\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_correct = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        preds = logits.argmax(1)\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "print(f\"Final Test Accuracy: {test_correct/len(test_loader.dataset)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "ax1nTemPtyyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ym0r2B3tyvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdFmsk8Gtytb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5wztpMHtyrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcqgBsdNtyoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1Dw0LCvtymS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpT9rjTxtykN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAKGFujgtyh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leY-vBaKE9pp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}